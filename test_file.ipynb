{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import daytime\n",
    "import re \n",
    "from scipy.stats.stats import pearsonr\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt \n",
    "import timeit\n",
    "import pickle \n",
    "import scipy \n",
    "from cvxopt import matrix, solvers \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import losses\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################################\n",
    "############# CLEAN THE DATASET TO EXTRACT TRAJECTORIES ############################################################################\n",
    "####################################################################################################################################\n",
    "\n",
    "def data_processing():\n",
    "    print(\"Loading the raw data file... \")\n",
    "    # IMPORTING THE RAW DATA OF SUBOPTIMAL POLICY\n",
    "    df = pd.read_csv('raw_data.txt', sep='\\t', engine='python', nrows=1000000, encoding= 'utf-16')\n",
    "    print(\"/*************/\")\n",
    "    print(\"File loaded... \")\n",
    "\n",
    "    # FUNCTIIONS USED TO CLEAN THE ABOVE RAW DATA \n",
    "    def read_data(file, enc='utf16', delim='\\t', decim='.'):\n",
    "        with open(file, newline='', encoding=enc) as datafile:\n",
    "            return pd.read_csv(datafile, skiprows=0, header=0, delimiter=delim, decimal=decim, low_memory=False)\n",
    "\n",
    "\n",
    "    def load_pkl(file):\n",
    "        with open(file, 'rb') as inp:\n",
    "            return pickle.load(inp)\n",
    "\n",
    "\n",
    "    def save_pkl(var, file):\n",
    "        with open(file, 'wb') as output:\n",
    "            pickle.dump(var, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "    def running_mean(x, n):\n",
    "        cumsum = np.cumsum(np.insert(x, 0, 0))\n",
    "        return (cumsum[n:] - cumsum[:-n]) / float(n)\n",
    "\n",
    "    def running_min(x, n):\n",
    "        min_list = pd.DataFrame(x).rolling(n).min()\n",
    "        return min_list.loc[(n-1):]\n",
    "\n",
    "    def running_max(x, n):\n",
    "        max_list = pd.DataFrame(x).rolling(n).max()\n",
    "        return max_list.loc[(n-1):]\n",
    "\n",
    "    def moving_avg_no_pad(pointList, winWidth):\n",
    "        cumsum, moving_aves = [0], []\n",
    "        #cumsum = [0] + list(accumulate(pointList))\n",
    "        cumsum = np.cumsum(np.insert(pointList, 0, 0))\n",
    "        for i, x in list(enumerate(pointList, 1)):\n",
    "            if winWidth % 2 == 0:\n",
    "                if i <= winWidth // 2:\n",
    "                    moving_ave = (cumsum[i + winWidth // 2 - 1] - cumsum[0]) / (i + winWidth // 2 - 1)\n",
    "                    moving_aves.append(moving_ave)\n",
    "                elif i > len(pointList) - winWidth // 2 + 1:\n",
    "                    moving_ave = (cumsum[len(pointList)] - cumsum[i - winWidth // 2 - 1]) / (len(pointList) - i + winWidth // 2 + 1)\n",
    "                    moving_aves.append(moving_ave)\n",
    "                else:\n",
    "                    moving_ave = (cumsum[i + winWidth // 2 - 1] - cumsum[i - winWidth // 2 - 1]) / winWidth\n",
    "                    moving_aves.append(moving_ave)\n",
    "            else:\n",
    "                if i <= winWidth // 2:\n",
    "                    moving_ave = (cumsum[i + winWidth // 2] - cumsum[0]) / (i + winWidth // 2)\n",
    "                    moving_aves.append(moving_ave)\n",
    "                elif i > len(pointList) - winWidth // 2:\n",
    "                    moving_ave = (cumsum[len(pointList)] - cumsum[i - winWidth // 2 - 1]) / (len(pointList) - i + winWidth // 2 + 1)\n",
    "                    moving_aves.append(moving_ave)\n",
    "                else:\n",
    "                    moving_ave = (cumsum[i + winWidth // 2] - cumsum[i - winWidth // 2 - 1]) / winWidth\n",
    "                    moving_aves.append(moving_ave)\n",
    "        return moving_aves\n",
    "\n",
    "    def range_sign(x, pos_thres=0):\n",
    "        if x > pos_thres:\n",
    "            return 1\n",
    "        elif x < -pos_thres:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "\n",
    "    def data_statistics_preprocess(df, target_column='Inc Electrical Antenna Tilt (deg)',drop_columns=[], pick_columns=[], split=0.7, reward_limit=20):\n",
    "        df = df.drop(columns=['CGI'])\n",
    "        # df = df.dropna(how='any')\n",
    "        # Theoretical range of angle is considered as (0-25)\n",
    "        df['Electrical Antenna Tilt (deg)'] = df['Electrical Antenna Tilt (deg)'].apply(\n",
    "        lambda x: float(x) / 25)\n",
    "        # Theoretical range of throughput is considered as (4,300,000 Kbps)\n",
    "        df['Average UE PDCP DL Throughput (Kbps)'] = df['Average UE PDCP DL Throughput (Kbps)'].apply(\n",
    "        lambda x: float(x) / 4300000)\n",
    "        # Theoretical range of number of calls is considered as (3,200,000)\n",
    "        df['Num Calls'] = df['Num Calls'].apply(\n",
    "        lambda x: float(x) / 3200000)\n",
    "        # Theoretical range of number of calls is considered as (-1 to 499)\n",
    "        df['Time Advance Overshooting Factor'] = df['Time Advance Overshooting Factor'].apply(\n",
    "        lambda x: float(x) / 500)\n",
    "        # Theoretical range of number of calls is considered as (100)\n",
    "        df['Inter Site Distance (Km)'] = df['Inter Site Distance (Km)'].apply(\n",
    "        lambda x: float(x) / 100)\n",
    "\n",
    "        df['Var Electrical Antenna Tilt (deg)'] = df['Var Electrical Antenna Tilt (deg)']\n",
    "\n",
    "        # Theoretical range of number of calls is considered as (100)\n",
    "        df['Inc Electrical Antenna Tilt (deg)'] = df ['Inc Electrical Antenna Tilt (deg)'].apply(\n",
    "        lambda x: np.sign(x))\n",
    "        # Percentages normalised by 100\n",
    "        df['Low RSRP Samples Rate Edge (%)'] = df['Low RSRP Samples Rate Edge (%)'].apply(\n",
    "        lambda x: float(x) / 100)\n",
    "        df['Number of Times Interf (%)'] = df['Number of Times Interf (%)'].apply(\n",
    "        lambda x: float(x) / 100)\n",
    "        df['Number of Cells High Overlap High Rsrp Src Agg (%)'] = df['Number of Cells High Overlap High Rsrp Src Agg (%)']\\\n",
    "        .apply(lambda x: float(x) / 100)\n",
    "        df['Number of Cells High Overlap High Rsrp Tgt Agg (%)'] = df['Number of Cells High Overlap High Rsrp Tgt Agg (%)']\\\n",
    "        .apply(lambda x: float(x) / 100)\n",
    "        df['PDCCH CCE High Load (%)'] = df['PDCCH CCE High Load (%)'].apply(lambda x: float(x) / 100)\n",
    "\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        return df\n",
    "\n",
    "\n",
    "    def split_df(df, inputs=None, outputs=None):\n",
    "        if outputs is None:\n",
    "            raise Exception('Output cannot be None when splitting'.format())\n",
    "        if inputs is None:\n",
    "            return df.drop(outputs), df[outputs]\n",
    "        return df[inputs], df[outputs]\n",
    "\n",
    "\n",
    "    # OBTAIN THE CLEAN VERSION OF THE RAW DATA \n",
    "    clean_data = data_statistics_preprocess(df)\n",
    "\n",
    "\n",
    "    print(\"columns:\", clean_data.columns)\n",
    "\n",
    "\n",
    "    cols = [\"Electrical Antenna Tilt (deg)\",\n",
    "    \"Average UE PDCP DL Throughput (Kbps)\",\n",
    "    \"Num Calls\",\n",
    "    \"Low RSRP Samples Rate Edge (%)\",\n",
    "    \"FUZZY_LOW_RSRP_SAMPLES_EDGE_HIGH\",\n",
    "    \"Number of Times Interf (%)\",\n",
    "    \"FUZZY_NUM_TIMES_INTERF_HIGH\",\n",
    "    \"Time Advance Overshooting Factor\",\n",
    "    \"FUZZY_OSF_HIGH\",\n",
    "    \"Number of Cells High Overlap High Rsrp Src Agg (%)\",\n",
    "    \"FUZZY_NUM_CELLS_HIGH_OVERLAP_SRC_HIGH\",\n",
    "    \"Number of Cells High Overlap High Rsrp Tgt Agg (%)\",\n",
    "    \"FUZZY_NUM_TIMES_HIGH_OVERLAP_TGT_HIGH\",\n",
    "    \"Inter Site Distance (Km)\",\n",
    "    \"PDCCH CCE High Load (%)\",\n",
    "    \"Inc Electrical Antenna Tilt (deg)\",\n",
    "    \"Var Electrical Antenna Tilt (deg)\",\n",
    "    \"Reward_Weighted_Delta_Driver_HighLevel_KPIs\"]\n",
    "\n",
    "\n",
    "    clean_data = clean_data[cols]\n",
    "    clean_data = clean_data.dropna(how='any') \n",
    "\n",
    "    print(\"Shape of clean data:\", clean_data.shape)\n",
    "\n",
    "\n",
    "    # Create state space \n",
    "    def create_state(df):\n",
    "        row_list = []\n",
    "        row_list2 = []\n",
    "        for index, rows in df.iterrows():\n",
    "            my_list =[rows.iloc[1],\n",
    "               rows.iloc[2],\n",
    "               rows.iloc[3],\n",
    "               rows.iloc[4],\n",
    "               rows.iloc[5],\n",
    "               rows.iloc[6],\n",
    "               rows.iloc[7],\n",
    "               rows.iloc[8],\n",
    "               rows.iloc[9],\n",
    "               rows.iloc[10],\n",
    "               rows.iloc[11],\n",
    "               rows.iloc[12],\n",
    "               rows.iloc[13],\n",
    "               rows.iloc[14]]\n",
    "            row_list.append(my_list) \n",
    "            row_list2.append(rows.iloc[17])\n",
    "        df['state'] = row_list\n",
    "        df['action'] = row_list2\n",
    "        rl_df = df[[\"state\", \"action\", \"Reward_Weighted_Delta_Driver_HighLevel_KPIs\"]]\n",
    "        rl_df = rl_df.rename(columns={ \"Reward_Weighted_Delta_Driver_HighLevel_KPIs\":\"reward\"})\n",
    "        return rl_df\n",
    "\n",
    "\n",
    "    newdf = create_state(clean_data)\n",
    "    newdf = newdf.reset_index()\n",
    "    newdf = newdf.drop(columns=['index'])\n",
    "    print(\"Shape of new df:\", newdf.shape)\n",
    "\n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################################\n",
    "#################### WRITE A DUMMY SIMULATOR #######################################################################################\n",
    "####################################################################################################################################\n",
    "\n",
    "NUM_OF_EPOCHS = 50\n",
    "\n",
    "def simulator(newdf):\n",
    "    # This simulator basically takes input as a state s and action a and spits out the next state s' i.e the new state is 'a' is taken in state 's'. \n",
    "    print(\"Creating the (s, a, r, s') pairs...\")\n",
    "    ls = []\n",
    "    l = len(newdf)\n",
    "    for index, row in newdf.iterrows():\n",
    "        if index!= l-1:\n",
    "            ls.append(newdf.iloc[index+1, 0])\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    newdf.drop(newdf.tail(1).index,inplace=True)\n",
    "    newdf['next_state'] = ls\n",
    "\n",
    "    print(\"Printing the new dataframe with (s, a, r, s') trajectories...\")\n",
    "    print(\"----------\")\n",
    "    print(newdf.columns)\n",
    "\n",
    "    # Creating the training data \n",
    "    X = []\n",
    "    for index, rows in newdf.iterrows():\n",
    "        r = [rows[0], [rows[1]]]\n",
    "        r = sum(r, [])\n",
    "        X.append(r)\t\n",
    "    y = newdf.iloc[:,3]\n",
    "\n",
    "    X_stack, y_stack = np.stack(X, axis=0), np.stack(y, axis=0)\n",
    "\n",
    "    print(\"Creating the model for simulator...\")\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=len(newdf.iloc[0,0]) + 1, activation= 'relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(5, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(len(newdf.iloc[0,0])))\n",
    "    model.compile(loss='logcosh', optimizer= Adam(lr= 0.0001))\n",
    "\n",
    "    print(\"Started training...\")\n",
    "    model.fit(X_stack, y_stack, epochs=NUM_OF_EPOCHS, verbose=1)\n",
    "    print(\"Simulator trained to predict s' from (s,a)...\")\n",
    "\n",
    "    model.save(\"transition_model_test.h5\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the raw data file... \n",
      "/*************/\n",
      "File loaded... \n",
      "columns: Index(['Date', 'ECell UserLabel', 'ENODEB Name', 'TAC', 'Colocated ECell',\n",
      "       'Template', 'Rule', 'Azimuth', 'Latitude', 'Longitude',\n",
      "       ...\n",
      "       'RELAT_new_DELTA_PDCCH_HIGH_LOAD', 'RELAT_new_DELTA_RETAINABILITY_DCR',\n",
      "       'REL_FUZZY_LOW_RSRP_EDGE', 'REL_FUZZY_OVERLAP_SRC',\n",
      "       'REL_FUZZY_OVERLAP_TGT', 'REL_FUZZY_INTERF', 'REL_FUZZY_OSF',\n",
      "       'RELAT_new_NUM_CALLS', 'Reward_Weighted_Delta_Driver_KPIs',\n",
      "       'Reward_Weighted_Delta_Driver_HighLevel_KPIs'],\n",
      "      dtype='object', length=356)\n",
      "Shape of clean data: (239319, 18)\n",
      "Shape of new df: (239319, 3)\n"
     ]
    }
   ],
   "source": [
    "newdf = data_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the (s, a, r, s') pairs...\n",
      "Printing the new dataframe with (s, a, r, s') trajectories...\n",
      "----------\n",
      "Index(['state', 'action', 'reward', 'next_state'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def create_data(newdf):\n",
    "\t# This simulator basically takes input as a state s and action a and spits out the next state s' i.e the new state is 'a' is taken in state 's'. \n",
    "\tprint(\"Creating the (s, a, r, s') pairs...\")\n",
    "\tls = []\n",
    "\tl = len(newdf)\n",
    "\tfor index, row in newdf.iterrows():\n",
    "\t    if index!= l-1:\n",
    "\t        ls.append(newdf.iloc[index+1, 0])\n",
    "\t    else:\n",
    "\t        break\n",
    "\tnewdf.drop(newdf.tail(1).index,inplace=True)\n",
    "\tnewdf['next_state'] = ls\n",
    "\tprint(\"Printing the new dataframe with (s, a, r, s') trajectories...\")\n",
    "\tprint(\"----------\")\n",
    "\tprint(newdf.columns)\n",
    "\treturn newdf\n",
    "\n",
    "newdf = create_data(newdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newdf.state[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the (s, a, r, s') pairs...\n",
      "Printing the new dataframe with (s, a, r, s') trajectories...\n",
      "----------\n",
      "Index(['state', 'action', 'reward', 'next_state'], dtype='object')\n",
      "Creating the model for simulator...\n",
      "Started training...\n",
      "Epoch 1/50\n",
      "239317/239317 [==============================] - 6s 26us/step - loss: 0.0134\n",
      "Epoch 2/50\n",
      "239317/239317 [==============================] - 6s 24us/step - loss: 0.0113\n",
      "Epoch 3/50\n",
      "239317/239317 [==============================] - 6s 24us/step - loss: 0.0112\n",
      "Epoch 4/50\n",
      "239317/239317 [==============================] - 6s 24us/step - loss: 0.0111\n",
      "Epoch 5/50\n",
      "239317/239317 [==============================] - 6s 24us/step - loss: 0.0111\n",
      "Epoch 6/50\n",
      "239317/239317 [==============================] - 6s 24us/step - loss: 0.0110\n",
      "Epoch 7/50\n",
      "239317/239317 [==============================] - 6s 24us/step - loss: 0.0110\n",
      "Epoch 8/50\n",
      "239317/239317 [==============================] - 6s 24us/step - loss: 0.0110\n",
      "Epoch 9/50\n",
      "239317/239317 [==============================] - 6s 24us/step - loss: 0.0110\n",
      "Epoch 10/50\n",
      "239317/239317 [==============================] - 6s 24us/step - loss: 0.0109\n",
      "Epoch 11/50\n",
      "239317/239317 [==============================] - 6s 24us/step - loss: 0.0109\n",
      "Epoch 12/50\n",
      "239317/239317 [==============================] - 6s 24us/step - loss: 0.0109\n",
      "Epoch 13/50\n",
      "239317/239317 [==============================] - 6s 24us/step - loss: 0.0109\n",
      "Epoch 14/50\n",
      "239317/239317 [==============================] - 6s 26us/step - loss: 0.0109\n",
      "Epoch 15/50\n",
      "239317/239317 [==============================] - 6s 24us/step - loss: 0.0109\n",
      "Epoch 16/50\n",
      "239317/239317 [==============================] - 6s 24us/step - loss: 0.0109\n",
      "Epoch 17/50\n",
      "239317/239317 [==============================] - 6s 24us/step - loss: 0.0108\n",
      "Epoch 18/50\n",
      "239317/239317 [==============================] - 6s 24us/step - loss: 0.0108\n",
      "Epoch 19/50\n",
      "239317/239317 [==============================] - 6s 24us/step - loss: 0.0108\n",
      "Epoch 20/50\n",
      "239317/239317 [==============================] - 6s 24us/step - loss: 0.0108\n",
      "Epoch 21/50\n",
      "239317/239317 [==============================] - 6s 24us/step - loss: 0.0108\n",
      "Epoch 22/50\n",
      "239317/239317 [==============================] - 6s 24us/step - loss: 0.0108\n",
      "Epoch 23/50\n",
      "239317/239317 [==============================] - 6s 24us/step - loss: 0.0108\n",
      "Epoch 24/50\n",
      "239317/239317 [==============================] - 6s 24us/step - loss: 0.0108\n",
      "Epoch 25/50\n",
      "239317/239317 [==============================] - 6s 24us/step - loss: 0.0108\n",
      "Epoch 26/50\n",
      "239317/239317 [==============================] - 6s 24us/step - loss: 0.0108\n",
      "Epoch 27/50\n",
      "239317/239317 [==============================] - 6s 25us/step - loss: 0.0108\n",
      "Epoch 28/50\n",
      "239317/239317 [==============================] - 6s 24us/step - loss: 0.0108\n",
      "Epoch 29/50\n",
      "239317/239317 [==============================] - 6s 24us/step - loss: 0.0108\n",
      "Epoch 30/50\n",
      "239317/239317 [==============================] - 6s 24us/step - loss: 0.0108\n",
      "Epoch 31/50\n",
      "239317/239317 [==============================] - 6s 24us/step - loss: 0.0108\n",
      "Epoch 32/50\n",
      "239317/239317 [==============================] - 6s 24us/step - loss: 0.0108\n",
      "Epoch 33/50\n",
      "239317/239317 [==============================] - 6s 24us/step - loss: 0.0108\n",
      "Epoch 34/50\n",
      "239317/239317 [==============================] - 6s 25us/step - loss: 0.0107\n",
      "Epoch 35/50\n",
      "239317/239317 [==============================] - 6s 26us/step - loss: 0.0107\n",
      "Epoch 36/50\n",
      "239317/239317 [==============================] - 6s 24us/step - loss: 0.0107\n",
      "Epoch 37/50\n",
      "239317/239317 [==============================] - 6s 24us/step - loss: 0.0107\n",
      "Epoch 38/50\n",
      "239317/239317 [==============================] - 6s 25us/step - loss: 0.0107\n",
      "Epoch 39/50\n",
      "239317/239317 [==============================] - 6s 25us/step - loss: 0.0107\n",
      "Epoch 40/50\n",
      "239317/239317 [==============================] - 6s 24us/step - loss: 0.0107\n",
      "Epoch 41/50\n",
      "239317/239317 [==============================] - 6s 24us/step - loss: 0.0107\n",
      "Epoch 42/50\n",
      "239317/239317 [==============================] - 6s 25us/step - loss: 0.0107\n",
      "Epoch 43/50\n",
      "239317/239317 [==============================] - 6s 24us/step - loss: 0.0107\n",
      "Epoch 44/50\n",
      "239317/239317 [==============================] - 6s 24us/step - loss: 0.0107\n",
      "Epoch 45/50\n",
      "239317/239317 [==============================] - 6s 24us/step - loss: 0.0107\n",
      "Epoch 46/50\n",
      "239317/239317 [==============================] - 6s 24us/step - loss: 0.0107\n",
      "Epoch 47/50\n",
      "239317/239317 [==============================] - 6s 24us/step - loss: 0.0107\n",
      "Epoch 48/50\n",
      "239317/239317 [==============================] - 6s 25us/step - loss: 0.0107\n",
      "Epoch 49/50\n",
      "239317/239317 [==============================] - 6s 24us/step - loss: 0.0107\n",
      "Epoch 50/50\n",
      "239317/239317 [==============================] - 6s 25us/step - loss: 0.0107\n",
      "Simulator trained to predict s' from (s,a)...\n"
     ]
    }
   ],
   "source": [
    "transition_model = simulator(newdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0042143069767441865, 0.0431975, 0.21715, 0.0, 0.022640000000000004, 0.0, 0.00051, 0.0, 0.0034000000000000002, 0.0, 0.00618, 0.0, 0.026269999999999998, 0.0, 0.0203775593]\n"
     ]
    }
   ],
   "source": [
    "stack_ = [newdf.state[0], [newdf.action[0]]]\n",
    "stack_ = reduce(operator.concat, stack_)\n",
    "print(stack_)\n",
    "stack_ = np.asarray(stack_)\n",
    "a = [stack_]\n",
    "a = np.asarray(a)\n",
    "res = transition_model.predict(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if os.path.isfile(\"transition_model.h5\"):\n",
    "    print(\"hi\")\n",
    "else:\n",
    "    print(\"ho\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.3621565e-03, 5.6585886e-02, 5.4282612e-01, 4.7278780e-01,\n",
       "        1.5290713e-02, 7.3866114e-02, 4.2882180e-03, 2.4689813e-01,\n",
       "        3.3692457e-03, 1.6849816e-02, 3.2768613e-03, 1.2006344e-02,\n",
       "        2.7062554e-02, 3.4150388e-04]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "act = newdf.next_state[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
